{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyLZW9xDonW-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 데이터 불러오기\n",
        "df = pd.read_csv('/content/drive/My Drive/soundAI/df_concat.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhsOikbLpMDO",
        "outputId": "fc2f0e6b-6411-45ff-dfe0-7105f31b4cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자열을 실제 텐서로 변환하는 함수\n",
        "def convert_to_tensor(tensor_str):\n",
        "    tensor_list = ast.literal_eval(tensor_str)\n",
        "    tensor = torch.tensor(tensor_list)\n",
        "    return tensor\n",
        "\n",
        "# 'feature' 열의 문자열을 실제 텐서로 변환하여 다시 저장\n",
        "df['feature'] = df['feature'].apply(convert_to_tensor)"
      ],
      "metadata": {
        "id": "TK3BeiKEpQQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 노이즈 추가 함수 정의\n",
        "def add_noise(tensor, noise_level=0.01):\n",
        "    noise = np.random.normal(0, noise_level, tensor.shape)\n",
        "    return tensor + noise\n",
        "\n",
        "\n",
        "# 시간 축 이동 함수 정의\n",
        "def time_shift(tensor, shift_max=2):\n",
        "    shift = np.random.randint(-shift_max, shift_max)\n",
        "    return np.roll(tensor, shift, axis=1)\n",
        "\n",
        "# 시간 축 스케일링 함수 정의\n",
        "def time_stretch(tensor, stretch_factor=0.2):\n",
        "    length = tensor.shape[1]\n",
        "    stretched_length = int(length * (1 + stretch_factor))\n",
        "    stretched_tensor = np.zeros((tensor.shape[0], stretched_length))\n",
        "    for i in range(tensor.shape[0]):\n",
        "        stretched_tensor[i] = np.interp(np.linspace(0, length, stretched_length), np.arange(length), tensor[i])\n",
        "    return stretched_tensor\n",
        "\n",
        "    # 데이터 증강 함수\n",
        "def augment_data(df, num_augmentations=10, noise_level=0.01):\n",
        "    augmented_features = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        feature = np.array(row['feature'])\n",
        "        label = row['label']\n",
        "\n",
        "        augmented_features.append(feature)  # 원본 데이터 추가\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "        for _ in range(num_augmentations - 1):\n",
        "            augmented_feature = add_noise(feature, noise_level)\n",
        "            augmented_feature = time_shift(augmented_feature)\n",
        "            augmented_feature = time_stretch(augmented_feature)\n",
        "            augmented_features.append(augmented_feature)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    augmented_df = pd.DataFrame({\n",
        "        'feature': augmented_features,\n",
        "        'label': augmented_labels\n",
        "    })\n",
        "    return augmented_df\n",
        "\n",
        "# 데이터 증강 실행\n",
        "augmented_df = augment_data(df)"
      ],
      "metadata": {
        "id": "Z0ZchcJBpXWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩 함수 정의\n",
        "def pad_feature(feature, target_shape=(1025, 130), pad_value=-1):\n",
        "    current_shape = feature.shape\n",
        "    padded_feature = np.full(target_shape, pad_value)\n",
        "    if current_shape[1] > target_shape[1]:\n",
        "        padded_feature[:, :] = feature[:, :target_shape[1]]\n",
        "    else:\n",
        "        padded_feature[:, :current_shape[1]] = feature\n",
        "    return padded_feature\n",
        "\n",
        "# 각 행마다 패딩 적용\n",
        "augmented_df['feature'] = augmented_df['feature'].apply(lambda x: pad_feature(np.array(x)))\n",
        "df['feature'] = df['feature'].apply(lambda x: pad_feature(np.array(x)))"
      ],
      "metadata": {
        "id": "kAFIYmtzpZ0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 매핑\n",
        "unique_labels = np.sort(augmented_df['label'].unique())\n",
        "label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}\n",
        "\n",
        "# 매핑을 적용하여 레이블 변환\n",
        "augmented_df['label'] = augmented_df['label'].map(label_mapping)\n",
        "df['label'] = df['label'].map(label_mapping)\n",
        "\n",
        "# 데이터 및 레이블 준비\n",
        "X_train = np.stack(augmented_df['feature'].values)\n",
        "y_train = augmented_df['label'].values\n",
        "\n",
        "X_test = np.stack(df['feature'].values)\n",
        "y_test = df['label'].values\n",
        "\n",
        "# 데이터 형태 조정 (2D CNN 입력 형태로 맞춤)\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]\n",
        "\n",
        "# 레이블 원-핫 인코딩\n",
        "num_classes = len(np.unique(y_train))\n",
        "y_train_categorical = to_categorical(y_train, num_classes)\n",
        "y_test_categorical = to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "RA7hVF5lpbRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2D CNN 모델 정의\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(1025, 130, 1)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "YmP-rPgmpc4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Learning rate scheduler 정의\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "\n",
        "# 모델 요약 출력\n",
        "model.summary()\n",
        "\n",
        "# 모델 훈련\n",
        "history = model.fit(X_train, y_train_categorical, validation_data=(X_test, y_test_categorical), epochs=50, batch_size=32, callbacks=[reduce_lr])\n",
        "\n",
        "# 모델 평가\n",
        "loss, accuracy = model.evaluate(X_test, y_test_categorical)\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "# 정확도와 손실 값을 시각화하는 함수\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy over Epochs')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss over Epochs')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# 정확도와 손실 값 시각화\n",
        "plot_history(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMJvRo0gLTJk",
        "outputId": "cfbbf32c-727e-4d02-b6d3-f6e94490125a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 1023, 128, 32)     320       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 1023, 128, 32)     128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 511, 64, 32)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 511, 64, 32)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 509, 62, 64)       18496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 509, 62, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 254, 31, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 254, 31, 64)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 252, 29, 128)      73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 252, 29, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 126, 14, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 126, 14, 128)      0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 124, 12, 256)      295168    \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 124, 12, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 62, 6, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 62, 6, 256)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 95232)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               48759296  \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 512)               2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 91)                46683     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 49197787 (187.67 MB)\n",
            "Trainable params: 49195803 (187.67 MB)\n",
            "Non-trainable params: 1984 (7.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**그래프 분석**\n",
        "\n",
        "1.\n",
        "훈련 정확도(Train Accuracy):\n",
        "\n",
        "훈련 정확도는 빠르게 상승하여 거의 100%에 도달했습니다. 이는 모델이 훈련 데이터에 대해 매우 잘 맞춰지고 있음을 나타냅니다.\n",
        "\n",
        "2. 검증 정확도(Validation Accuracy):\n",
        "\n",
        "검증 정확도는 초기에는 상승하다가 약 20-30 에포크 사이에서 급격히 하락하고, (30에 끊어야하나) 이후 매우 불안정한 모습을 보입니다.\n",
        "\n",
        "3. 훈련 손실(Train Loss):\n",
        "\n",
        "훈련 손실은 거의 0에 가까워지고, 훈련 데이터에 대해 매우 잘 맞춰지고 있음을 나타냅니다.\n",
        "\n",
        "4. 검증 손실(Validation Loss):\n",
        "\n",
        "검증 손실은 초반에 낮다가 약 20-30 에포크 이후 급격히 상승하며 매우 불안정한 모습을 보입니다."
      ],
      "metadata": {
        "id": "cv0aqErCt99q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cSvzKMJDub0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 모델은 과적합이 발생하고 있습니다.\n",
        "이를 해결하기 위해 다음과 같은 조치를 고려할 수 있습니다:\n",
        "\n",
        "1. 조기 종료(Early Stopping): 검증 손실이 증가하기 시작할 때 훈련을 중지합니다.\n",
        "\n",
        "2. 데이터 증강(Data Augmentation): 훈련 데이터를 다양하게 만들어\n",
        "모델이 더 잘 일반화할 수 있도록 합니다.\n",
        "\n",
        "3. 정규화(Regularization): 드롭아웃(Dropout)이나 가중치 정규화(Weight Regularization) 기법을 사용하여 모델의 복잡도를 줄입니다.\n",
        "\n",
        "4. 훈련 데이터 양 증가: 더 많은 훈련 데이터를 수집하여 모델이 더 다양한 데이터를 학습할 수 있도록 합니다.\n",
        "이러한 방법들을 통해 과적합 문제를 완화시킬 수 있습니다."
      ],
      "metadata": {
        "id": "ytCTT1kmuAeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 데이터 불러오기\n",
        "df = pd.read_csv('/content/drive/My Drive/soundAI/df_concat.csv')\n",
        "\n",
        "\n",
        "# 문자열을 실제 텐서로 변환하는 함수\n",
        "def convert_to_tensor(tensor_str):\n",
        "    tensor_list = ast.literal_eval(tensor_str)\n",
        "    tensor = torch.tensor(tensor_list)\n",
        "    return tensor\n",
        "\n",
        "# 'feature' 열의 문자열을 실제 텐서로 변환하여 다시 저장\n",
        "df['feature'] = df['feature'].apply(convert_to_tensor)\n",
        "\n",
        "# 노이즈 추가 함수 정의\n",
        "def add_noise(tensor, noise_level=0.01):\n",
        "    noise = np.random.normal(0, noise_level, tensor.shape)\n",
        "    return tensor + noise\n",
        "\n",
        "\n",
        "# 시간 축 이동 함수 정의\n",
        "def time_shift(tensor, shift_max=2):\n",
        "    shift = np.random.randint(-shift_max, shift_max)\n",
        "    return np.roll(tensor, shift, axis=1)\n",
        "\n",
        "# 시간 축 스케일링 함수 정의\n",
        "def time_stretch(tensor, stretch_factor=0.2):\n",
        "    length = tensor.shape[1]\n",
        "    stretched_length = int(length * (1 + stretch_factor))\n",
        "    stretched_tensor = np.zeros((tensor.shape[0], stretched_length))\n",
        "    for i in range(tensor.shape[0]):\n",
        "        stretched_tensor[i] = np.interp(np.linspace(0, length, stretched_length), np.arange(length), tensor[i])\n",
        "    return stretched_tensor\n",
        "\n",
        "    # 데이터 증강 함수\n",
        "def augment_data(df, num_augmentations=10, noise_level=0.01):\n",
        "    augmented_features = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        feature = np.array(row['feature'])\n",
        "        label = row['label']\n",
        "\n",
        "        augmented_features.append(feature)  # 원본 데이터 추가\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "        for _ in range(num_augmentations - 1):\n",
        "            augmented_feature = add_noise(feature, noise_level)\n",
        "            augmented_feature = time_shift(augmented_feature)\n",
        "            augmented_feature = time_stretch(augmented_feature)\n",
        "            augmented_features.append(augmented_feature)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    augmented_df = pd.DataFrame({\n",
        "        'feature': augmented_features,\n",
        "        'label': augmented_labels\n",
        "    })\n",
        "    return augmented_df\n",
        "\n",
        "# 데이터 증강 실행\n",
        "augmented_df = augment_data(df)\n",
        "\n",
        "# 패딩 함수 정의\n",
        "def pad_feature(feature, target_shape=(1025, 130), pad_value=-1):\n",
        "    current_shape = feature.shape\n",
        "    padded_feature = np.full(target_shape, pad_value)\n",
        "    if current_shape[1] > target_shape[1]:\n",
        "        padded_feature[:, :] = feature[:, :target_shape[1]]\n",
        "    else:\n",
        "        padded_feature[:, :current_shape[1]] = feature\n",
        "    return padded_feature\n",
        "\n",
        "# 각 행마다 패딩 적용\n",
        "augmented_df['feature'] = augmented_df['feature'].apply(lambda x: pad_feature(np.array(x)))\n",
        "df['feature'] = df['feature'].apply(lambda x: pad_feature(np.array(x)))\n",
        "\n",
        "# 레이블 매핑\n",
        "unique_labels = np.sort(augmented_df['label'].unique())\n",
        "label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}\n",
        "\n",
        "# 매핑을 적용하여 레이블 변환\n",
        "augmented_df['label'] = augmented_df['label'].map(label_mapping)\n",
        "df['label'] = df['label'].map(label_mapping)\n",
        "\n",
        "# 데이터 및 레이블 준비\n",
        "X_train = np.stack(augmented_df['feature'].values)\n",
        "y_train = augmented_df['label'].values\n",
        "\n",
        "X_test = np.stack(df['feature'].values)\n",
        "y_test = df['label'].values\n",
        "\n",
        "# 데이터 형태 조정 (2D CNN 입력 형태로 맞춤)\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]\n",
        "\n",
        "# 레이블 원-핫 인코딩\n",
        "num_classes = len(np.unique(y_train))\n",
        "y_train_categorical = to_categorical(y_train, num_classes)\n",
        "y_test_categorical = to_categorical(y_test, num_classes)\n",
        "\n",
        "# 2D CNN 모델 정의\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(1025, 130, 1)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dq8mpSzCLdf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Learning rate scheduler 정의\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "\n",
        "# 모델 요약 출력\n",
        "model.summary()\n",
        "\n",
        "# 모델 훈련\n",
        "history = model.fit(X_train, y_train_categorical, validation_data=(X_test, y_test_categorical), epochs=50, batch_size=32, callbacks=[reduce_lr])\n",
        "\n",
        "# 모델 평가\n",
        "loss, accuracy = model.evaluate(X_test, y_test_categorical)\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "# 정확도와 손실 값을 시각화하는 함수\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy over Epochs')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss over Epochs')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# 정확도와 손실 값 시각화\n",
        "plot_history(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "urQPpX8RpedZ",
        "outputId": "b2884749-0fb1-4342-9c63-5a06f80a24fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 1023, 128, 32)     320       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 1023, 128, 32)     128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 511, 64, 32)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 511, 64, 32)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 509, 62, 64)       18496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 509, 62, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 254, 31, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 254, 31, 64)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 252, 29, 128)      73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 252, 29, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 126, 14, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 126, 14, 128)      0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 124, 12, 256)      295168    \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 124, 12, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 62, 6, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 62, 6, 256)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 95232)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               48759296  \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 512)               2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 91)                46683     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 49197787 (187.67 MB)\n",
            "Trainable params: 49195803 (187.67 MB)\n",
            "Non-trainable params: 1984 (7.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "30/46 [==================>...........] - ETA: 5s - loss: 0.0951 - accuracy: 0.9781"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-47e3750de931>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 모델 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_categorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_categorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 모델 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}